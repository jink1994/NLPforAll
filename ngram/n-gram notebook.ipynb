{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b275073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5262f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_reader(corpusfile, lexicon=None): \n",
    "    with open(corpusfile,'r') as corpus: \n",
    "        for line in corpus: \n",
    "            if line.strip():\n",
    "                sequence = line.lower().strip().split()\n",
    "                if lexicon: \n",
    "                    yield [word if word in lexicon else \"UNK\" for word in sequence]\n",
    "                else: \n",
    "                    yield sequence\n",
    "\n",
    "def get_lexicon(corpus):\n",
    "    word_counts = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        for word in sentence: \n",
    "            word_counts[word] += 1\n",
    "    return set(word for word in word_counts if word_counts[word] > 1)  \n",
    "\n",
    "\n",
    "\n",
    "def get_ngrams(sequence, n):\n",
    "    \"\"\"\n",
    "    Input: a list of strings\n",
    "    Output:a list of n-grams, where each n-gram is a Python tuple\n",
    "    \"\"\"\n",
    "    start_token = 'START'\n",
    "    end_token = 'STOP'\n",
    "    _sequence = copy.deepcopy(sequence) #use another sentence to manipulate so that adding tokens will not affect other operations\n",
    "    _sequence.insert(0,start_token)\n",
    "    _sequence.append(end_token)\n",
    "    if n>=3:\n",
    "        for i in range(n-2):\n",
    "            _sequence.insert(0,start_token)\n",
    "    \n",
    "    return [\n",
    "        (\n",
    "            tuple([_sequence[i-n+j] for j in range(n)])\n",
    "        )\n",
    "        for i in range(n, len(_sequence)+1)\n",
    "    ]\n",
    "\n",
    "\n",
    "class TrigramModel(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        corpusfile,\n",
    "        smooth_rate,\n",
    "        \n",
    "                ):\n",
    "    \n",
    "        # Iterate through the corpus once to build a lexicon \n",
    "        generator = corpus_reader(corpusfile)\n",
    "        self.lexicon = get_lexicon(generator)\n",
    "        self.lexicon.add(\"UNK\")\n",
    "        self.lexicon.add(\"START\")\n",
    "        self.lexicon.add(\"STOP\")\n",
    "        self.unigram_smoothing_rate=smooth_rate[0]\n",
    "        self.bigram_smoothing_rate=smooth_rate[1]\n",
    "        self.trigram_smoothing_rate=smooth_rate[2]\n",
    "    \n",
    "        # Now iterate through the corpus again and count ngrams\n",
    "        generator = corpus_reader(corpusfile, self.lexicon)\n",
    "        self.count_ngrams(generator) #this defines the uni/bi/tri dictionary for the corpus file\n",
    "        self.sum_of_unigram_frequency=sum(self.unigramcounts.values())\n",
    "\n",
    "    def count_ngrams(self, corpus):\n",
    "        \"\"\"\n",
    "        Input: corpus iterator\n",
    "        Output:dictionaries of unigram, bigram, and trigram counts\n",
    "        \"\"\"\n",
    "   \n",
    "        self.unigramcounts = defaultdict(int) \n",
    "        self.bigramcounts = defaultdict(int) \n",
    "        self.trigramcounts = defaultdict(int)\n",
    "\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            for unigram in get_ngrams(sentence,1):\n",
    "                self.unigramcounts[unigram]+=1\n",
    "            for bigram in get_ngrams(sentence,2):\n",
    "                self.bigramcounts[bigram]+=1\n",
    "            for trigram in get_ngrams(sentence,3):\n",
    "                self.trigramcounts[trigram]+=1\n",
    "\n",
    "    def raw_unigram_probability(self, unigram):\n",
    "        \"\"\"\n",
    "        Input: one unigram tuple\n",
    "        Output: raw (unsmoothed) unigram probability\n",
    "        \"\"\"\n",
    "        return self.unigramcounts[unigram]/self.sum_of_unigram_frequency #sum_of_unigram_frequency is the total number of tokens \n",
    "\n",
    "    def raw_bigram_probability(self, bigram):\n",
    "        \"\"\"\n",
    "        Input: one bigram tuple\n",
    "        Output: raw (unsmoothed) bigram probability\n",
    "        \"\"\"\n",
    "\n",
    "        all_appearance = self.unigramcounts[tuple((bigram[0],))]\n",
    "        if all_appearance==0:\n",
    "            return 0.0#return a probability of 0 if the conext is not seen in the training data\n",
    "        \n",
    "        return self.bigramcounts[bigram]/all_appearance\n",
    "    \n",
    "    def raw_trigram_probability(self,trigram):\n",
    "        \"\"\"\n",
    "        Input: one trigram tuple\n",
    "        Output: raw (unsmoothed) trigram probability\n",
    "        \"\"\"\n",
    "        all_appearance = self.bigramcounts[tuple((trigram[0],trigram[1]))]\n",
    "        if all_appearance==0:\n",
    "            return 0.0 #return a probability of 0 if the conext is not seen in the training data\n",
    "        return self.trigramcounts[trigram]/all_appearance\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def generate_sentence(self,t=20): \n",
    "        \"\"\"\n",
    "        Generate a random sentence from the trigram model. t specifies the\n",
    "        max length, but the sentence may be shorter if STOP is reached.\n",
    "        \"\"\"\n",
    "\n",
    "        previous2 = ['START','START']\n",
    "        end_token = 'STOP'\n",
    "\n",
    "        generated_sentence=[]\n",
    "        count=0\n",
    "        while count<t: #generated senetence has length limit of t\n",
    "            #generate the distribution of next_word\n",
    "            choice_list=[]\n",
    "            choice_weight=[]\n",
    "            for x,y,z in self.trigramcounts.keys():\n",
    "                if x==previous2[0] and y==previous2[1]:\n",
    "                    choice_list.append(tuple((x,y,z)))\n",
    "            a = sum([self.trigramcounts[i] for i in choice_list])\n",
    "            for i in choice_list:\n",
    "                choice_weight.append(self.trigramcounts[i]/a)\n",
    "\n",
    "            next_word = random.choices(choice_list,choice_weight)[0][2] #Sample the next word\n",
    "\n",
    "            if next_word !=end_token:\n",
    "                generated_sentence.append(next_word)\n",
    "                previous2.pop(0) \n",
    "                previous2.append(next_word)#update the previous word list\n",
    "            else:\n",
    "                break\n",
    "            count+=1\n",
    "        \n",
    "        return generated_sentence            \n",
    "\n",
    "    def smoothed_trigram_probability(self, trigram):\n",
    "\n",
    "        lambda1 = self.unigram_smoothing_rate\n",
    "        lambda2 = self.bigram_smoothing_rate\n",
    "        lambda3 = self.trigram_smoothing_rate\n",
    "\n",
    "        _unigram = tuple((trigram[2],)) #grab the last word in a trigram\n",
    "        _bigram = tuple((trigram[1],trigram[2]))#grab the last two words in a trigram\n",
    "        _trigram = trigram\n",
    "\n",
    "        result = lambda1*self.raw_unigram_probability(_unigram)+lambda2*self.raw_bigram_probability(_bigram)+lambda3*self.raw_trigram_probability(_trigram)\n",
    "\n",
    "        return result\n",
    "        \n",
    "    def sentence_logprob(self, sentence):\n",
    "        \"\"\"\n",
    "        Returns the log probability of an entire sequence.\n",
    "        \"\"\"\n",
    "        trigrams = get_ngrams(sentence,3)\n",
    "        sum_logp=0\n",
    "        for trigram in trigrams:\n",
    "            sum_logp+=math.log2(self.smoothed_trigram_probability(trigram))\n",
    "        \n",
    "        return sum_logp\n",
    "\n",
    "    def perplexity(self, corpus):\n",
    "        \"\"\"\n",
    "        Returns the perplexity of an entire corpus.\n",
    "        \"\"\"\n",
    "        sum=0\n",
    "        token_count = 0\n",
    "        for sentence in corpus:\n",
    "            sum+= self.sentence_logprob(sentence)\n",
    "            token_count += len(get_ngrams(sentence,1))#count the number of tokens in the testing data\n",
    "\n",
    "        return 2**(-sum/token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f3982",
   "metadata": {},
   "source": [
    "### Now train the model with your dataset.\n",
    "We are using the brown [corpus](https://www.kaggle.com/nltkdata/brown-corpus). <br>\n",
    "And I'm putting only the tokenized column version in the current path. <br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Play around the weights on smoothing to see what it does on the generated sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrigramModel(\n",
    "    './brown_train.txt',\n",
    "    [1/3.0,1/3.0,1/3.0]) #this is the coefficient list used for linear interpolation. (unigram,bigram,trigram)\n",
    "dev_corpus = corpus_reader('./brown_train.txt', model.lexicon)\n",
    "print(f'The perplexity for this corpus is: {model.perplexity(dev_corpus)}')\n",
    "for i in range(10):\n",
    "    print(f'The NO.{i}th sentence generated is: {model.generate_sentence(t=50)}.')\n",
    "#Generate 10 sentences using the current model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
